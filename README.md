# ğŸ§  Bayesian Multimodal Fusion for Depression Classification  
[![Bayesian Machine Learning](https://img.shields.io/badge/Bayesian-Machine_Learning-blue)](https://github.com/yourrepo)  
**A probabilistic approach for detecting Major Depressive Disorder (MDD) using Bayesian inference with speech and EEG features.**  

##  **Why This Matters**
- Depression affects over 300 million people worldwide and is the leading cause of disability.
- In the U.S. alone, 21 million adults experience at least one major depressive episode annually.
- Missed or late diagnosis leads to suffering and lost productivityâ€”costing the global economy over $1 trillion per year. 

âœ… Our goal is to **improve depression diagnosis with AI**â€”but in a way thatâ€™s **trustworthy and uncertainty-aware**.  


## ğŸ“Œ **Overview**
This project applies **Bayesian Machine Learning** techniques to classify depression using **speech embeddings from Wav2Vec 2.0** and **EEG-based neural activity features**. The goal is to **quantify uncertainty** in predictions and enhance model interpretability for **clinical decision-making**.

- ğŸ“Š **Data Shape:** 1,774 samples, 823 features  
- ğŸ§  **Multimodal Data:** Speech + EEG  

ğŸš€ **Key Bayesian Methods Used:**  
- **Bayesian Neural Networks (BNN)** (with uncertainty-aware predictions)  
- **Bayesian Gaussian Process Classification (GPC)**  
- **Bayesian Logistic Regression (BLR)**  
- **KL Weight Annealing & Threshold Tuning** (for optimizing precision and recall)  

---

## ğŸ¯ **Motivation**
Major Depressive Disorder (MDD) is often **underdiagnosed** due to subjective assessments.  
ğŸ”¹ **Why Bayesian Learning?** It provides:  
âœ… **Uncertainty estimation** (crucial for clinical AI)  
âœ… **Better generalization** over small datasets  
âœ… **Robustness to noise** in speech & EEG data  

---

## ğŸ“Š **Methodology**
### **1ï¸âƒ£ Data Processing**
We use **speech data** (Wav2Vec 2.0) and **EEG signals** as features.

ğŸ“Œ **Preprocessing Steps:**  
- **Speech:** Resampling, silence trimming, volume normalization, embedding extraction  
- **EEG:** Filtering, noise removal, dimensionality reduction (PCA)  

---

### **2ï¸âƒ£ Bayesian Neural Network (BNN)**
We used a **probabilistic BNN** with **KL weight annealing** and **threshold tuning** to improve classification performance.  

ğŸ”¢ **KL Weight Annealing:**  
We gradually increased the KL divergence weight over time to **stabilize training and reduce overfitting**.  

ğŸ“Œ **Final Results:**  
| **Threshold** | **Accuracy** | **Precision** | **Recall** | **F1-Score** |
|--------------|-------------|--------------|-------------|--------------|
| **0.50**  | **55.19%** | **73.00%** | **16.46%** | **27.12%** |
| **0.65**  | **65.00%** | **53.33%** | **100.00%** | **69.57%** âœ… |

ğŸš€ **Best Threshold:** **0.65** â†’ **Maximizes Recall (100%) while maintaining a good F1-score.**  

ğŸ–¼ **Precision-Recall Curve:**  
![Precision-Recall Curve](path/to/your_image.png)

---

### **3ï¸âƒ£ Bayesian Gaussian Process Classification (GPC)**
Gaussian Process Classification is a **non-parametric Bayesian approach** that provides **probabilistic outputs** and measures **uncertainty effectively**.

ğŸ“Œ **GPC Results:**  
âœ… **Accuracy: 94.12%**  
âœ… **Robust classification with uncertainty estimation**  
âš ï¸ **Potential overfitting â€“ needs external validation**  

---

### **4ï¸âƒ£ Bayesian Logistic Regression (BLR)**
Bayesian Logistic Regression helps **analyze feature importance** and **quantify uncertainty** in the model.

ğŸ“Œ **Key Observations:**  
- Some weight distributions were **multimodal**, indicating **uncertainty in feature contributions**.  
- Certain features had a **94% HDI including zero**, suggesting they may not strongly influence predictions.  

ğŸ–¼ **Weight Posterior Distributions:**  
![Posterior Distributions](path/to/your_image2.png)

---

## ğŸ›  **Code Implementation**
### **1ï¸âƒ£ Bayesian Neural Network (BNN)**
```python
threshold = 0.65  # Optimized threshold
with torch.no_grad():
    y_pred_probs = bnn(X_test)
    y_pred = (y_pred_probs > threshold).float()
```

### **2ï¸âƒ£ KL Weight Annealing in PyMC**
```python
with pm.Model() as bnn_model:
    kl_weight = pm.Data("kl_weight", 0.1)  # Start small, increase over time
    w = pm.Normal("w", mu=0, sigma=1, shape=(X_train.shape[1],))
    logits = pm.math.dot(X_train, w)
    pm.Bernoulli("y_obs", logit_p=logits, observed=y_train)
```

## ğŸ“ˆ **Results Summary**
We didnâ€™t just test one modelâ€”we tried three different **Bayesian approaches** to find the best method for detecting depression.  

| **Model** | **Accuracy** | **Key Benefit** |
|-----------|------------|----------------|
| **Gaussian Process Classifier (GPC)** | **94.12%** | âœ… Very confident predictions, but might overfit |
| **Bayesian Logistic Regression (BLR)** | **TBD** | âœ… Helps understand which features matter most |
| **Bayesian Neural Network (BNN - Threshold 0.50)** | **55.19%** | âœ… Uses KL Weight Annealing for uncertainty control |
| **BNN (Threshold 0.65)** | **65.00%** | âœ… Best balance between detecting depression and avoiding false alarms |

ğŸš€ **What we learned:**  
- **GPC** gives highly confident predictions but might overfit.  
- **BLR** is great for understanding feature importance.  
- **BNN (with KL Weight Annealing)** helped improve prediction stability.  
- **Optimizing the threshold (0.65)** helped balance **precision vs. recall** better.  

## ğŸ” **How Bayesian Machine Learning Works**
Bayesian Machine Learning is different from traditional deep learning because it **doesn't just predict a labelâ€”it predicts a probability distribution** over possible outcomes. This makes it ideal for **uncertainty-aware AI in clinical applications** like depression diagnosis.

### ğŸ“Œ **Key Bayesian Concepts Used in This Project**
1ï¸âƒ£ **Bayesian Priors: Adding Knowledge Before Seeing Data**
   - In Bayesian models, we set **priors** (initial beliefs) before seeing any data.
   - Example: In **Bayesian Logistic Regression (BLR)**, we assume model weights follow a **Normal(0,1)** distribution to prevent overfitting.

2ï¸âƒ£ **Posterior Distribution: Learning from Data**
   - After seeing data, Bayesian inference **updates our beliefs** to get the **posterior distribution**.
   - This is done using methods like **Variational Inference (VI)** or **Markov Chain Monte Carlo (MCMC)**.

3ï¸âƒ£ **Uncertainty Quantification: Why It Matters**
   - Unlike traditional models that just output **"Depressed" or "Not Depressed"**, Bayesian models also tell us **how confident** they are.
   - This is **crucial for healthcare AI** because it helps doctors **interpret ambiguous cases** instead of blindly trusting an AI.

### ğŸ¤– **Different Bayesian Methods We Used**
| **Model** | **How It Works** | **Best Use Case** |
|-----------|----------------|------------------|
| **Gaussian Process Classifier (GPC)** | Uses a kernel function to model uncertainty. | Works well for small datasets, but may overfit. |
| **Bayesian Logistic Regression (BLR)** | Uses Bayesian priors to estimate probabilities. | Helps understand which features matter most. |
| **Bayesian Neural Network (BNN)** | Adds uncertainty to deep learning by training weight distributions instead of single values. | Great for complex, high-dimensional data like speech + EEG. |

### ğŸ›  **Special Techniques We Used**
ğŸ”¹ **KL Weight Annealing** (in BNN): Prevents the model from converging too early to a bad solution.  
ğŸ”¹ **Threshold Optimization**: We experimented with different decision thresholds to balance **precision** and **recall**.  
ğŸ”¹ **Variational Inference (VI)**: Faster alternative to **MCMC** for training Bayesian models efficiently.

---

## ğŸš€ **Final Recommendation:**
- BNN + Threshold 0.65 provides the best trade-off between precision and recall.
- GPC is highly accurate but may need validation to check for overfitting.
- BLR helps in understanding feature importance.


## ğŸ“Œ **Future Work**
- ğŸ”¹ EEG Feature Integration Improvements ğŸ§ 
- ğŸ”¹ Hybrid Bayesian Models: Combine GPC, BLR, and BNN for multimodal fusion.
- ğŸ”¹ Clinical Validation: Test the framework on real-world depression screening data.
- ğŸ”¹ Improve BNN by adding **better priors** and testing **Hybrid Bayesian Models**!


## ğŸ“œ **Citations**

- Wav2Vec 2.0: https://arxiv.org/abs/2006.11477
- EEGPT: https://openreview.net/forum?id=lvS2b8CjG5
- EEG Dataset: https://www.nature.com/articles/s41597-022-01211-x
- Global Prevalence of Depression: The World Health Organization (WHO) reports that depression affects more than 300 million people worldwide and is the leading cause of disability. â€‹
- Prevalence in the United States: In the United States, approximately 8.4% of adults, equating to 21 million individuals, experience at least one major depressive episode annually. â€‹(en.wikipedia.org)
- Economic Impact: Depression and anxiety disorders result in the loss of approximately 12 billion working days each year, costing the global economy over $1 trillion annually. 

